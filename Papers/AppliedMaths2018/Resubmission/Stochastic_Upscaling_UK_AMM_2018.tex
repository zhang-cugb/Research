
% Copyright 2007, 2008, 2009 Elsevier Ltd 
% 
% This file is part of the 'Elsarticle Bundle'.
% --------------------------------------------- 
%
% It may be distributed under the conditions of the LaTeX Project Public
% License, either version 1.2 of this license or (at your option) any
% later version.  The latest version of this license is in
%    http://www.latex-project.org/lppl.txt
% and version 1.2 or later is part of all distributions of LaTeX
% version 1999/12/01 or later c.
%
% The list of all files belonging to the 'Elsarticle Bundle' is
% given in the file `manifest.txt'.
%
 
% Template article for Elsevier's document class `elsarticle'
% with harvard style bibliographic references
% SP 2008/03/01
%
%
%
% $Id: elsarticle-template-harv.tex 4 2009-10-24 08:22:58Z rishi $
%
%
%\documentclass[preprint,authoryear,12pt]{elsarticle}
\documentclass[preprint,12pt]{elsarticle}

% Use the option review to obtain double line spacing
%\documentclass[authoryear,preprint,review,12pt]{elsarticle}

% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
% for a journal layout:
%\documentclass[final,authoryear,1p,times]{elsarticle}
%\documentclass[final,authoryear,1p,times,twocolumn]{elsarticle}
%\documentclass[final,authoryear,3p,times]{elsarticle}
%\documentclass[final,authoryear,3p,times,twocolumn]{elsarticle}
%\documentclass[final,authoryear,5p,times]{elsarticle}
%\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the gra

%%graphis package for simple commands
%% \usepackage{graphics}
%\usepackage{cases}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
\usepackage{epsfig}
%\usepackage{subfig}
\usepackage{comment}

\usepackage{epstopdf}
\usepackage{pdflscape}
\usepackage{bm}
\usepackage{hyperref,url}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=red}

%% The amssymb package provides various useful mathematical symbols

\usepackage{amssymb,amsmath,array}
%The amsthm package provides extended theorem environments

\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[mathscr]{euscript}
%\usepackage{subfigure}
  
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

\usepackage{lscape}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\newcommand{\JGnote}[1]{\fbox{\parbox{\textwidth}{ \color{red} JG Note $\Rightarrow$ #1}}}
\newcommand{\BLnote}[1]{\fbox{\parbox{\textwidth}{ \color{green} BL Note $\Rightarrow$ #1}}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\green}{\textcolor{green}}
\newcommand{\yellow}{\textcolor{yellow}}
\newcommand{\frc}{\displaystyle\frac}
\newcommand{\PN}[2][error]{P$_{#1}$DG-P$_{#2}$}
\newcommand{\PNDG}[2][error]{P$_{#1}$DG-P$_{#2}$DG}
\newcommand{\eg}{{\it e.g., }} 
\newcommand{\ie}{{\it i.e., }}
\newcommand{\st}{{\it s.t., }}

\journal{Applied Mathematical Modelling}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}} 
%% \fntext[label3]{}

\title{A Reduced Order Model for Porous Media Flows which Couples Linear Regression with Principal Component Analysis (PCA) using SVD or HOSVD: Upscaling the Permeability Field for Multiphase Reservoir Flow Simulation}
\author[UoA]{B. Lashore\corref{cor1}}\ead{lashorebabatunde@yahoo.com} \author[UoA]{K. Christou} \author[UoA]{J.L.M.A. Gomes}
\cortext[cor1]{Corresponding author.}
\address[UoA]{Mechanics of Fluids, Soils \& Structures Research Group \\ School of Engineering, University of Aberdeen, UK}


\begin{abstract}

Representation of heterogeneous properties (\ie permeability in this case) within a large system, is a challenge that cuts across many industries that deal with flows in porous media. This is because it is impossible to obtain the value of permeability at every point in realistic domains. And, even if this was possible, it would be extremely challenging to get the computing resources to make use of all available data values. This challenge gave birth to upscaling, a model order reduction method.

In this paper, a single realization of a permeability field was upscaled by first projecting the permeability field into a principal component analysis space (using SVD for 2D field and HOSVD for 3D fields). Then linear regression was employed within each of the principal component spaces to obtain approximate representations of the initial principal component. The new principal components are then recombined to for a new permeability field with a reduced order. This PCA-linear regression method was compared to traditional industry-standard upscaling techniques (\eg arithmetic and harmonic averaging) and to a stochastic-based (probability density function, PDF) method of upscaling, to highlight its benefits/performance.

It was shown that the new upscaling technique retained the heterogenous nature of the BaseCase (\ie high-resolution configuration). Additionally, the upscaling technique did not require {\it a priori} understanding of similar permeability blocks within the existing domain.

\end{abstract}



\begin{keyword} %% keywords here, in the form: keyword \sep keyword
Principal Component Analysis (PCA) and Linear Regression \sep Singular Value Decomposition (SVD) and Higer Order Singular Value Decomposition (HOSVD) \sep Upscaling\sep Reduced Order Models (ROM), \sep Permeability field \sep Heterogeneity
\end{keyword}
 
\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Introduction}\label{section:intro}
Naturally occuring rock formations are inherently heterogeneous, which means that rock properties such as permeability, pore spaces (\ie porosity) and pore throat vary spatially at different length scales. Heterogeneity of rock formations is relevant in calculations of transport and storage of fluids in porous media (\ie multiphase flow in subsurface rocks). Therefore, the representation of such heterogeneous properties is of great importance to oil and gas (\ie reservoir management), carbon capture and storage (CCS, \ie CO$_2$ transportation and storage in subsurface rocks) and waste management (\ie remediation of contaminated soil).

In essence, rock heterogeneity makes it impossible to obtain reliable and accurate spatial information about geological properties throughout the domain (\ie uncertainty problem). Additionally, due to limitations in computing resources, it is neither efficient nor possible to use exact values of geological properties at all spatial coordinates of the domain to perform fine-grid simulations (\ie scaling problem) \cite{chen_2006,miller_1998,Renard_1997}. This is particularly true for large geological domains which span several kilometres. It is possible to overcome these challenges by a family of techniques called {\it upscaling}. Upscaling techniques solve both uncertainty and scaling problems by replacing discrete geological and fluid properties of detailed high resolution domains with coarse descriptions (low resolution) of these properties \cite{Vereecken_2007}.

Upscaling techniques which best preserve statistical properties and flow dynamics behaviour of the high resolution domain give the best representation of heterogeneous properties. There are several upscaling techniques which fall under different categories \cite{Hasting_2001, Renard_1997, Szymkiewicz_2013}. Traditional upscaling techniques are deterministic in nature, however over the last decade, techniques classed as stochastic have received great attention from academic and industrial porous media communities worldwide \cite{Guilleminot_2012, Ravalec-Dupin_2010, Verwoerd_2009}. One of the reasons for this is because stochastic upscaling techniques are robust enough to handle multiphase flow in porous media and they are able to address scaling and uncertainity problems.

\medskip
{\it Upscaling} is a term commonly used in the oil and gas sector and is often referred, throughout the applied mathematics and engineering communities, as a form of model order reduction (or reduced order models, MOR/ROM). Depending on the context in which MOR is used, it can simply be referred as dimensionality reduction. The definition of MOR also depends on the context it is being used, but it suffices to state that MOR captures the essential features of a system, structure or parameter \cite{Schilders2008}. This implies that an original system exists with more detailed information. Figure~\ref{fig:IllustrationMOR} presents an interpretation of MOR concept in which a graphical representation of a rabbit is reconstructed with a reduced number of facets (rhs of this illustration).  

\citet{Schilders2008} provided a historical perspective on the development of Model Order Reduction, from the work of Fourier in 1807 which approximated a function with a few trigonometric terms, to the work of \citet{Odabasioglu_1998} in 1998 which introduced Passive Reduced-order Interconnect Macromodeling Algorithm (PRIMA). Truncated Balanced Realization  is another MOR method developed by \citeauthor{Moore_1981}~\cite{Moore_1981}, which introduced the principal component analysis (PCA) \cite{Hotelling_1933} and \citeauthor{Golub1970}'s \cite{Golub1970} algorithm for solving singular value decomposition (SVD). Hankel-norm reduction \cite{Glover_1984} and Proper Orthogonal Decomposition (POD) \cite{Sirovich_1987} are also MOR methods discussed in the references published in 1984 and 1987, respectively. The asymptotic waveform evaluation (AWE) \cite{Pillage_1990} was the first MOR method based on Krylov subspace technique, this method was followed by Pade-via-Lanczos method \cite{Feldmann_1995} and the PRIMA. In this work, upscaling is performed through linear regression in the PCA space which is obtained with SVD or HOSVD for 2D and 3D cases, respectively.

\bigskip
This work focuses on reducing the order of permeability distribution in porous media flow simulations. Section~\ref{section:mathematical_model} introduces the mathematical model used to describe the relevant physics for multi-fluid flow through porous media and highlights the relevance of the permeability field. The mathematical model is discretised by a novel high-order accurate control volume finite element method (CVFEM) \cite{Gomes_2017} which couples both the fine resolution and upscaled representation of the permeability field. Pre-processing statistical properties and post-simulation multiphase flow behaviour are investigated to assess the various upscaling techniques used. Four upscaling techniques are investigated in this work and they are named Arithmetric mean, Harmonic mean, probability density function (PDF) and PCA (using SVD for 2D models and HOSVD for 3D model) coupled with linear regression.

In Section~\ref{section:pca_linreg}, fundamental mathematics of SVD and associated properties are presented. Then, the concept of spaces is explained to give the reader the understanding necessary to appreciate the transformations and projections involved in this work. Finally, Section~\ref{section:pca_linreg} describes the reduced order model reduction for the permeability field within the principal component space. Section~\ref{section:model_simulation} is dedicated to model description and simulation. It starts by describing the high resolution (\ie base case) on which the four upscaling techniques are applied to. A brief summary of the pre-processing step for each of the models is also presented. Section~\ref{section:results_discussion} provides the results and further discussion on the results of the simulations, it also gives further interpretation to the result obtained from the SVD upscaling technique. Finally, Conclusions are drawn in Section~\ref{section:conclusion}.

\subsection{Brief Overview of Upscaling}\label{section:overview_upscaling}

Upscaling methods replace discrete geological and fluid properties of detailed high resolution domains with coarse descriptions (low resolution) of these properties \cite{Vereecken_2007}. This subject of upscaling heterogenous properties has interested industries (\ie oil and gas, CCS and waste management) as well as the academic community for over $7$ decades.

Those interested in large-scale reservoirs or ground water generally take a two-stage geostatistical approach. In the first stage, information from seismic data, well data and analogous outcrops is used to model the large scale heterogeneities associated with facies. In the second stage, statistical models such as continuous multi-variate Gaussian field is used to model the rock properties of the facies \cite{Ewing_1997}. In addition to seismic and well data used in the previous stage, core data can be used as a sources of information for the statistical properties used to determine the random field in this stage.

Several studies were carried out on the upscaling of permeability fields \cite{Christie_2001SPE10Model, Ewing_1997, Hasting_2001, Indelman_1993, King1996, Vereecken_2007, Wen_1996, Yeo2001}. \citet{Cardwell_1945} investigated arithmetic and weighted averages as upscaling techniques for calculating a single equivalent permeability for oil reservoirs with varying permeabilities. The general conclusion reached from this study was that the equivalent permeability lies between the harmonic and the arithmetic average.  

\citet{Yeo2001} discussed the accuracy of renormalization method for upscaling 2D hydraulic conductivity fields for two cases. Both cases employed the $4$-quadrant model (see Section~\ref{subsection:model} for details) set-up. The first case had low permeablitity in all the blocks except the bottom right block which had high permeability, while the second case employed the checkboard style described in Section~\ref{subsection:model}. Different conductivity ratios were also investigated for the two cases. The conclusion drawn was that renormalization worked fairly well for the first case, but very poorly for the second case.

\citet{Renard_1997} provided a comprehensive review of the various techniques for upscaling. They found out that the techniques classed broadly under deterministic, stochastic and heuristic complemented each other rather than antagonising each other. \citet{Renard_1997} then recommended that it is better to select a technique which provides block permeabilities instead of one which provides uniform effective permeability.

\citet{Christie_2001SPE10Model} provided $2$ sets of problems to compare the upscaling and upgridding techniques of different simulators. The paper does not describe in any detail the upscaling techniques used by the different simulators, but it provides a complete dataset for running simulations.


\subsection{PCA, SVD, POD, KLT and Reservoir Characterisation}\label{section:reservoir_char}

PCA, SVD and POD are closely related. In the petroleum engineering community where these techniques are used for reservoir characterization, they are sometimes used interchangable, and are normally referred to as the Karhunen-Loeve Transformation (KLT) \cite{Jafarpour_2009}.

A lot of the recent research in the field of petroleum engineering field which use these techniques, focus on the problem of history matching \cite{Afra_2013, Insuasty_2017, Jafarpour_2009, Tavakoli_2010, Xiao_2018}. History matching is the process of building or adjusting one or more sets of models for a reservoir, such that the model(s) closely reproduces the past behaviour (flow rate, production volume or pressures) of a reservoir, and hopefully predict its future behaviour as well \cite{Rwechungura_2011}. It is important to note that the model(s) referred to here could be a numerical model representing the system of parameters (\ie{variables}) which characterise the reservoir, or simply a model for parameterizing one of more components (\ie{variables}) of the system.

\citet{Jafarpour_2009} compares KLT to discrete cosine transform (DCT) and goes on to state that the DCT is computationally more efficient than the KLT while being almost as accurate. \citet{Tavakoli_2010} noted that history matching calculation of sensitivities for all production data with respect to system parameters (\ie{variables}) is not feasible. Hence, they (\citet{Tavakoli_2010}) explored a new parameterization for reducing the number of variables based on the principal right singular vectors of the dimensionless sensitivity matix.

Perhaps, the works of \citet{Afra_2013}, \citet{Afra_2016}, \citet{Afra_2014}, \citet{Insuasty_2017} are more closely related to the one performed here. However, similar to the work of \citet{Jafarpour_2009}, they are more related to comparing several realizations than reducing the dimensionality of the dataset for a single realization. The work of \citet{Xiao_2018} is also closely related to that performed here and interestingly, they used the same CVFEM simulator used in this work. However, like the work of \citet{Tavakoli_2010}, the \citeauthor{Xiao_2018} were more interested in reducing the number of variable used to describe the system.

Essentially, the recent developments in the field of petroleum engineering described here make use of SVD to select the most suitable realization from multiple realizations or to reduce the representative system variables. However, the work described here is concerned with reducing dimensionality of the dataset for a single realization of one of the system variable (\ie{permeability}).


\section{Multi-Fluid Flow Model}\label{section:mathematical_model}

In this work, a control volume finite element method (CVFEM) formulation is used to discretise and solve the set of multi-fluid flow equations. Continuity equations are embedded into the pressure equation to enforce mass conservation and the exact force balance (extended Darcy equations). The numerical formulation employs an implicit algorithm with respect to time which is less restrictive than the traditional industry-standard implicit-pressure-explicit-saturation (IMPES) scheme \cite{aziz_1986,geiger_2004}.

The formulation used in this work uses a dual consistent CVFEM representation which is embedded in novel families of triangular and tetrahedral finite element-pairs, \PN[n]{m} and \PNDG[n]{m}. In this element-pairs velocity is represented by $n^{\rm th}$-order polynomials that are discontinuous across elements, while pressure is represented by $m^{\rm th}$-order polynomials that may be either continuous (\PN[n]{m}) or discontinuous (\PNDG[n]{m}) across elements. Most properties of \PNDG[n]{m} element-pairs are similar to those in the \PN[n]{m} element-pairs but allows a representation in which pressure, saturation and other solution variables (\eg temperature, concentrations etc) are discontinuous across finite element boundaries \cite{adam_2016, salinas_2018}.

Immiscible multi-fluid flows in porous media may be represented by the extended Darcy equation,
\begin{equation}\label{e:darcy_eqn}
  \mathbf{q}_{\alpha} = \mathbf{u}_{\alpha}S_\alpha=
  -\frac{\mathcal{K}_{{r}_\alpha}\mathbf{K}}{\mu_{\alpha}}\left(
  \nabla p_{\alpha} - {\mathbf{s}_{u}}_{\alpha} \right),
\end{equation}
where $\alpha$ designates a phase, $\mathbf{q}_{\alpha}$ is the $\alpha$-th phase Darcy velocity and $\mathbf{u}_{\alpha}$ is the saturation-weighted Darcy velocity. $\mu_{\alpha}$, $p_{\alpha}$, $\rho_{\alpha}$, and $\mathbf{s}_{{u}_\alpha}$ are the phase dynamic viscosity, pressure, density and source term, which may include gravity and/or capillarity, respectively. $\mathcal{K}_{{r}_\alpha}\left(S_{\alpha}\right)$ is the phase relative permeability, which is a function of the phase saturation $S_{\alpha}\left(\mathbf{r},t\right)$, which in turn is a function of the vector position $\mathbf{r}$, and time, $t$.

In addition to the extended Darcy equation (Eqn.~\ref{e:darcy_eqn}), saturation conservative equations,
\begin{equation}
  \phi\displaystyle\frac{\partial S_{\alpha} }{\partial t} + \nabla
  \cdot \left( {\mathbf u}_{\alpha} S_{\alpha}\right) =
  s_{cty,\alpha},
  \label{saturation_equation}
\end{equation}
are discretised in space with CV basis functions, and in time with the $\theta$-method \cite{pavlidis_2013b}. In Eqn.~\ref{saturation_equation}, $\phi$ is the porosity and $s_{cty}$ is a source term.

\medskip
The discretised saturation and Darcy equations are solved using a multigrid-like approach as described by \citeauthor{pavlidis2016}~\cite{pavlidis2016}. The numerical formulation is fully described by \citeauthor{Gomes_2017}~\cite{Gomes_2017} (see also \cite{adam_2016, salinas2015}). These numerical methods are embedded in the next-generation flow simulator Fluidity/IC-FERST model software\footnote{\href{http://multifluids.github.io}{http://multifluids.github.io}} (a full description of the model can be found in \cite{Gomes_2017, jackson_2013}). Although, the numerical formulation used here is relatively new, it is not the focus of this work. The result and discussion section (Section \ref{section:results_discussion}) refers to \citet{dawe_2008} for validation of the numerical formulation with respect to the 2D simulation. However, more extensive validation tests can be found in the aforementioned references \cite{adam_2016, Gomes_2017,jackson_2013, pavlidis2016, salinas2015}.


\medskip
Finally, $\mathbf{K}\left(\mathbf{r}\right)$, the focus of this work, is the absolute permeability tensor of the porous medium and it is a function of the vector position $\mathbf{r}$. $\mathbf{K}$ is prescribed in the system of governing conservative equations, \ie it is an indepedent input variable. Since $\mathbf{K}$ is a petrophysical rock property that varies spatially, it cannot be determined at every spatial coordinate. And, even if this information is available at every spatial coordinate, it would require immense computer resources to process during flow simulations. This is the reason upscaling is required for processing permeability variables. Upscaling can be performed from the continuum scale, across the micro scale ($mm$), local scale and meso scale ($m$) to the field scale ($km$)\cite{ECMI_2004}. The upscaling discussed here is from the micro scale also known as the Darcy scale.


\section{PCA coupled with Linear Regression}\label{section:pca_linreg}

\subsection{Principal Component Spaces and Model Order Reduction for the permeability field}\label{subsection:pcspaces}
  
\subsubsection{Understanding the permeability space}\label{subsubsection:visualization_permspace}
Permeability can be represented as tensor quantity, but for the purpose of the following explanation (and the remaining of this paper), it is assumed that permeability is a scalar quantity. Furthermore, the simulator prescribes the permeability field by assigning permeability values to the centre of element-pairs \cite{Christou_2018} which discretises the simulation domain.

Permeability values assigned in the centre of the element/cell represents values of permeability for the associated element $\left(\text{\ie P}_{0}\text{DG element-pair}\right)$. For simplicity, in the remaining part of this sub-section, a $2D$ geometry is assumed where the geometry is discritized by a square grid with the value for each square element assigned in the centre of the element. This explanation allow one to easily visualize this permeability space because it is defined by the cartesian coordinates system.

The ROM method discussed here is concerned with reducing the volume of permeability data within the permeability space. Consider for instance a small square which is discretized to contain four squares such that $4$ data points are defined in the middle of the four squares. The permeability space of $4$ can be reduced to a smaller value of $1$ through an averaging process such as arithmetic, harmonic or geometric mean. This type of reduction is the focus of this paper, albeit the main focus of this paper is achieving this reduction through linear regression in the principal component spaces. 

\subsubsection{Understanding the principal component spaces}\label{subsubsection:visualization_pcspaces}
Essentially, the existing permeability space is transformed with SVD for 2D permeability field and HOSVD for 3D permeability fields, to principal component spaces (PCS). Within the PCS, traditional PCA dimensionality reduction can be optionally performed (\ie{feature selection or elimination}, see Section~\ref{subsubsection:pca_dimred_linreg} for more details), before interpolation is used to implement further data reduction in the principal component spaces. The reduced data in the PCS is then transformed back into the permeability space to obtain a reduced permeability space. An algorithm which impliments the procedure described here, is presented in Section~\ref{subsubsection:svdcase_preprocess_algorithm}

The PCS is a bit challenging to visualize because it represents projections which are embedded in the SVD/HOSVD factors of the original permeability field. An aid for visualizing the PCS for a 2D permeability field is to write out the $\mathbf{U}$, $\mathbf{\Sigma}$ and $\mathbf{V^{\intercal}}$ matrices (or SVD factors) and consider the singular vectors (\ie columns in $\mathbf{U}$ and rows in $\mathbf{V^{\intercal}}$) as separate PCS. A similar intuition can be extended to HOSVD for 3D permeability fields. The decomposition of a matrix into its factors using SVD  \cite{Cline_2007, Tharwat_2016} or HOSVD \cite{Kolda_2009, Lathauwer_2000} is covered in details in several published materials. However, for completeness, the appendix contains a brief introduction to SVD (\ref{subsection:svd_brief}) and HOSVD (\ref{subsection:hosvd_brief}).

\subsubsection{Reduced Order Model/Principal Component Space Reduction}\label{subsubsection:pca_dimred_linreg}
For the PCA-Linear regression (PCA-LinReg) reduced order model of the permeability field, all the work is done in the principal component spaces. This can be completed in two steps, which is exemplified for the 2D model thus:
\begin{enumerate}
  \item The first step is the principal component analysis (PCA) \cite{Hotelling_1933, Tharwat_2016} using the SVD method, where the singular values are examined and, based on a pre-defined criteria, $z$ singular values are selected, starting from $\sigma_{1}$ and in a decreasing order, where $z \leq r$.

Note that, in the case with $z = r$, it can be assumed that step 1 has been ignored. However, when $z < r$, the first $z$ columns of $\mathbf{U}$ and the first $z$ rows of $\mathbf{V^{\intercal}}$ are extracted from $\mathbf{U}$ and $\mathbf{V^{\intercal}}$ to form new matrices $\mathbf{U_{new}}$ and $\mathbf{V^{\intercal}_{new}}$ respectively. The selected $z$ singular values also form a new diagonal matrix, $\mathbf{\Sigma_{new}}$ .

\item The second step involves interpolation within each singular vector (column in $\mathbf{U}$ and row in $\mathbf{V^{\intercal}}$) to reduce the data points in each column of  $\mathbf{U}$ and in $\mathbf{V^{\intercal}}$. From this step,  $\mathbf{U_{new2}}$ and $\mathbf{V^{\intercal}_{new2}}$ are obtained. The reduced permeability field is calculated by multiplying  $\mathbf{U_{new2}}$, $\mathbf{\Sigma_{new}}$ and $\mathbf{V^{\intercal}_{new2}}$.

\end{enumerate}

It is trivial to show that these two steps can be replicated for a 3-D model using HOSVD to obtain the PCA space.\\

Finally on the PCS, it is worth noting that two methods exist for obtaining PCA space\cite{Tharwat_2016} , namely, the co-variance method and the SVD method. \citet{Tharwat_2016} goes on to describe both methods in details. In this work, the authors have opted to use the SVD method to obtain the PCA space. This is because the singular vectors which make up the PCS have a strong correlation to the coordinate axis they represent. This was exploited in making the assumption that a linear relationship exist for the data in each of the PC spaces, hence the linear regression performed to reduce the data in the PCS. 

\subsection{Principal Component Analysis Versus Linear Regression}\label{subsection:pcs_vs_linreg}

\citet{AndrewNg_2018} was quoted to have said ``PCA is not linear regression, and despite some cosmetic similarity, these are actually totally different algorithms''. The main differences between these two mathematical methods is summarised in the Table~\ref{table:pca_vs_linreg} below and can also be deduced from the figure~\ref{fig:PCA_is_not_LinReg}.

\begin{table}[h!]
\centering
\begin{tabular}{|m{1em} |m{14em} |m{14em}|} 
 \hline
  & PCA & Linear Regression \\ [0.5ex] 
 \hline
 1 & Determines the best vector (or set of vectors) which compresses the data points by minimizing the orthogonal distance to the points & Approximates the best fit line for the points by minimizing the error \\
 \hline
 2 & Essentially, it forms a projection of the original points onto the new vector space & Predicts $y$ from $x$ through a function $f(x_1,x_2,....,x_n) = y$ \\
 \hline
 3 & Not used for predictions, as original feature space (\ie $x_1$ and $x_2$) give way to a new reduced dimension. Cordinates $x_1, x_2,...,x_n$ give way to $z_1,....,z_k$ where $k \leq n$ & \\[1ex]
 \hline
\end{tabular}
\caption{Table comparing PCA with Linear Regression}
\label{table:pca_vs_linreg}
\end{table}


\section{Model Description and Simulation Setup}\label{section:model_simulation}

As previosly mentioned, four upscaling techniques are investigated in this work. Arithmetic and harmonic means are used to obtain the first two upscaled representation and these two are classed as deterministic techniques. The third is a randomly generated permeability field with Gaussian distribution, prescribed by a probability density function (PDF) which was obtained from a domain discretised with high-resolution mesh with known permeability distribution (\ie base case). The fourth upscaling technique is the crux of this research, it introduces a reduced order model which couples PCA and linear regression.

\subsection{Model Description}\label{subsection:model}
The BaseCase  (\ie fine mesh resolution) model for the permeabiliy field builds on the $2 \times 2$ block, $4$ quadrant model used initially by \citet{Cardwell_1945} and later by \citet{Yeo2001} and \citet{dawe_2008}. These authors designed the permeability field for the quadrants in a checkerboard style which means that diagonally opposite blocks retained the same permeability values with one set of diagonally opposite blocks having a higher permeability value compared to the other set. Hence, one set of diagonally opposite blocks are referred to as high permeability blocks while the other set is referred to as low permeability blocks.

The BaseCase permeability model used here retains the checkerboard desgined used by the recently named authors. Albeit, the permeability dataset for each block in the set of diagonally opposite blocks have similar value (\ie not exactly the same as in the papers referred to), although one set can still be referred to as a set of high permeability blocks while the other set is referred to as a set of low permeability blocks. Furthermore, within each block, the permeability field varies within a pre-defined range and consists of $1600$ permeability values. More details on the design of the permeability field for the BaseCase is available in section~\ref{subsubsection:basecase_preprocess_algorithm}. Figures~\ref{fig:PermFields}a and~\ref{fig:PermFields}b give a pictorial representation of the permeability field for the base case with the mesh discretization visible and invisible, respectively.  Using the labelling scheme used by \citet{dawe_2008}, the quadrant model block $K1,~K2,~K3$ and $K4$ are top left, bottom left, top right and bottom right, respectively.

As already mentioned, four upscaled permeability field models were obtained from the high-resolution (\ie the BaseCase) permeability field. When compared to the BaseCase model, all the upscaled models were upgridded to reduce the resolution of the permeability field (Fig.~\ref{fig:HiRes_LowRes_Mesh}). The first two upscaled models are arithmetic and harmonic averaging methods  (referred to as ArithMean and HarmMean, respectively).

In both sets of test-cases, averages are calculated for each block using the $1600$ data points from the blocks of the BaseCase to obtain a permeability value for each block in the upscaled models. This effectively homogenizes the permeability field in each block. The upscaled permeability field for the third and fourth cases are referred to as PDFCase and PCA-LinReg Case, respectively. And, as can be deduced from the names, these cases are based on probability density function (PDF) and SVD, respectively. The description for these permeability field models are presented in Sections~\ref{subsubsection:pdfcase_preprocess_algorithm} and ~\ref{subsubsection:svdcase_preprocess_algorithm} respectively. It is appropriate to note that although these models (PDFCase and PCA-LinReg Case) are upscaled, they retain heterogeneity within the blocks.

\subsubsection{BaseCase Pre-processing Algorithm}\label{subsubsection:basecase_preprocess_algorithm}
\begin{enumerate}[1.]
  \item A function is used to randomly generate uniform (\ie Gaussian) dataset for each block. The range for each block is as below:
  \begin{enumerate}[a)]
    \item \textbf{K1} : $700 - 900$ mD
    \item \textbf{K2} : $50 - 150$ mD
    \item \textbf{K3} : $100 - 200$ mD
    \item \textbf{K4} : $1000 - 1200$ mD
  \end{enumerate}                                                    
  \item The number of data points generated for each block is $1600$ and the data values are randomly allocated within each block in a structured pattern. Albeit, the simulation mesh is unstructured.
  \item Merge the dataset from different blocks into a single file retaining the original ``structured pattern'' (in step 1 above) for the randomly positioned dataset for each block. This file will be used by the CVFEM-based flow simulator to prescribe the permeability field for the base case.
\end{enumerate}

\subsubsection{PDFCase Pre-processing Algorithm}\label{subsubsection:pdfcase_preprocess_algorithm}
\begin{enumerate}[1.]
  \item The datasets from step one of the pre-processing algorithm for the base case (Section~\ref{subsubsection:basecase_preprocess_algorithm}) are used to calculate mean and standard deviation for each block of the permeability field.
  \item A function which generates a Gaussian dataset using arithmetic mean and standard deviation is used to stochastically produce a quarter of the data points from the BaseCase for each of the four blocks  (\ie $400$ data points for each block).
  \item The data values in the dataset for each block is randomly positioned within each block in a structured pattern. As in the base case, the simulation grid is unstructured.
  \item Finally, the structured/arranged dataset from each block is combined into a single file and then used by the flow simulator, which merge the permeability mapping to the mesh with the appropriate element-pairs.
\end{enumerate}

\subsubsection{2-D PCA (\ie{using SVD}) \& LinReg Case Pre-processing Algorithm}\label{subsubsection:svdcase_preprocess_algorithm}
\begin{enumerate}[1.]
  \item The permeability field obtained in step 3 of the pre-processing algorithm for the base case is factorized using SVD to obtain $\mathbf{U}, \Sigma,$ and $\mathbf{V}$.
  \item A selected number ($z$) of singular values are retained in decreasing order of magnitude to form a new diagonal matrix, $\Sigma_{new}$. (This step is optional, it is the conventional PCA dimensionality reduction)
  \item The first $z$ columns of $\mathbf{U}$ and $\mathbf{V}$ are selected to form new truncated matrices call $\mathbf{U_{new}}$ and $\mathbf{V_{new}}$ respectively. (This step is necessary only if the optional step in 2 above is executed)
  \item Within each column of $\mathbf{U_{new}}$ and $\mathbf{V_{new}}$, a linear interpolation is performed to form a new column with the number of data points in each column reduced to half of the original column. These new columns are recombined to form $\mathbf{U_{new2}}$ and $\mathbf{V_{new2}}$.
  \item The permeability field used for the SVD case is the matrix $\mathbf{A_{new2}}$, which is defined as the product of $\mathbf{U_{new2}} \mathbf{\Sigma_{new}} \mathbf{V_{new2}^{\intercal}}$. It is a permeability field with a quarter of the data values in the base case and therefore a permeability field with reduced order.  
\end{enumerate}


\subsubsection{3-D PCA (\ie{using HOSVD}) \& LinReg Case Pre-processing Algorithm}\label{subsubsection:hosvdcase_preprocess_algorithm}
The algorithm for the 3-D case is similar to that of the 2-D case described above. However:
\begin{enumerate}[1.]
\item In step 1 above, HOSVD is used for factorization instead of SVD. Therefore, a $3 x 3$ core tensor is obtained in place of the $\mathbf{\Sigma}$ term, while 3 factor matrices are obtained in place of $\mathbf{U}$ and $\mathbf{V}$.
  \item It goes without saying that the core tensor is treated as the $\mathbf{\Sigma}$ term, while the 3 factor matrices are treated similarly to $\mathbf{U}$ and $\mathbf{V}$ as described in Section \ref{subsubsection:svdcase_preprocess_algorithm} above.
\end{enumerate}
The authors found the updated Tensorly library package \cite{Tensorly_2018} helpful in implimenting and executing the algorithm for HOSVD.

\subsection{Simulations}\label{subsection:simulations}
For the simulations, each of the model's domain is initially fully saturated with a fluid (Phase 2), and a wetting phase (1) fluid is driven into the domain from the left hand side at a constant mass flow rate. No-flux boundary conditions were imposed on the upper and lower borders of each domain, while mixed fluids are recovered from the right-hand face of the domain. Relative permeability, $\mathcal{K}_{r\alpha}$ (Eqn.~\ref{e:darcy_eqn}), which is often expressed as a function of local, residual and maximum phase saturations prescribed in the pore rock matrix. In the simulations conducted for this work, the modified \citet{Brooks_1964} model was used \citep{alpak_1999},
\begin{eqnarray}
  \mathcal{K}_{rw}\left(S_{w}\right) &=& \mathcal{K}^{\circ}_{rw}\left[\frc{S_{w}-S_{w,irr}}{1-S_{w,irr}-S_{nw,r}}\right]^{n_{w}}, \label{Eqn:CoreyBrooks1}\\
  \mathcal{K}_{rnw}\left(S_{nw}\right) &=& \mathcal{K}^{\circ}_{rnw}\left[\frc{S_{nw}-S_{nw,r}}{1-S_{w,irr}-S_{nw,r}}\right]^{n_{nw}}, \label{Eqn:CoreyBrooks2}
\end{eqnarray}
where subscripts $w$ and $nw$ stand for wetting and non-wetting phases, respectively. $\mathcal{K}^{\circ}_{rw}$ and $\mathcal{K}^{\circ}_{rnw}$ are end-point relative permeability to wetting and non-wetting phases, $S_{w,irr}$ and $S_{nw,r}$ are irreducible wetting and residual non-wetting phase saturations, respectively. Exponents $n_{w}$ and $n_{nw}$ are both set to 2.

Figure~\ref{fig:PermFields} shows permeability mapping for all cases, while Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} show phase saturation distribution during fluid injection for time,
\begin{displaymath}
  t = 0.15,~0.30,~0.50,~1.15,~1.75\text{ and } 2.95 \text{ s for all cases.} 
\end{displaymath}

\section{Results and Discussion}\label{section:results_discussion}

Mesh resolution for the the BaseCase and upscaled cases are shown in Fig,~\ref{fig:HiRes_LowRes_Mesh}. Domain of the BaseCase (\ie high-resolution mesh) was discretised with $4112$ \PN[1]{1} triangular element-pairs, whilst in upscaled cases the domain was discretised with $728$ \PN[1]{1} triangular element-pairs. Therefore the resolution for the upscaled cases is reduced by a factor of approximately $5.6$.

Figure~\ref{fig:PermFields} compares the permeability distribution for all the models. As indicated in the figure's caption, the permeability legend in Fig.~\ref{fig:PermFields}(a) is representative for all the other figures (\ie Fig.~\ref{fig:PermFields}(b) to (f)). Additionally, Fig.~\ref{fig:PermFields}(b) to (f) show the cases without the element edges which discretize the domain.

This highlights the fact that the permeability distribution of the ArithMean and HarmMean cases, Fig.~\ref{fig:PermFields}(c) and (d), are homogeneous within each block, while PDFCase and PCA-LinReg cases, Fig.~\ref{fig:PermFields}(e) and (f), are heterogeneous. Given that the BaseCase is also heterogeneous, it can be stated that the model for the PDFCase and the PCA-LinReg Case give a better representation of the BaseCase. An additional consequence of the homogeneity in the ArithMeanCase and the HarmMeanCase is that the upgrid size has no influence on the outcome of their permeability distribution. Whereas, for the PDFCase and the PCA-LinReg Case, the permeability field will change if the upgrid resolution changes.

With respect to the PDFCase and for this specific resolution, it is important to note that Fig.~\ref{fig:PermFields}(e) is only one realiztion of infinitely many possible representations of the PDFCase. Meanwhile, the PCA-LinReg Case will retain this specific (\ie Fig.~\ref{fig:PermFields}(e)) permeability field for this specific resolution (\ie if linear interpolation is used, but this digresses from the main point and will be discussed in future work). Therefore, the permeability value at a specific point in the PDFCase is not directly related to the data value at the nearest point in the BaseCase because the randomly generated PDF dataset is also randomly arranged within each block. However, the data value at a point in the PCA-LinReg Case is directly related to the nearest data values with respect to the same point in the BaseCase, throught the interpolation which takes place in the PCS.

Furthermore, prior to the pre-processing required for evaluating the ArithMeanCase, HarmMeanCase and the PDFCase it is necessary to have an understanding of the whole domain and pre-define blocks with similar properties (\ie blocks $K1,~K2,~K3$ and $K4$) in this case. However, with the PCA-LinReg Case, knowledge of blocks with similar properties is not required prior to the upscaling. Albeit, one has to select the upgrid resolution carefully, if the resolution is too large then there is the risk of losing relevant data in the interpolation. The limits of the techniques used for the PCA-LinReg Case will be discussed in the author's future work.

Before going on to review the results of the simulations, it is beneficial to briefly consider the Buckley-Leverett and cross-flow behaviour which are be used to validate the results from this work. 

\citet{buckley_1942} in 1942 derived the Buckley-Leverett equation, an analytical (albeit simplified) two phase flow equation. The equation determined that a plane of constant saturation progresses uniformly through the domain during the flow simulation. The equation is referred to as "simplified" because of the assumptions made while deriving it, some of which were ignored in this work. Principally, this work ignores the fact that the equation deals with injection into an homogeneous permeability field while this work deals injection into two blocks ($K1$ and $K2$) with different permeabilities. Additionally, the permeability field in each block is heterogeneous. However, the general validation test or expectation from Buckley-Leverett's influence on this work, is that, the injected fluid saturation in $K1$ and $K2$ is expected to have a piston-shape front within those blocks, subject some inconsistency at the boundary between the two blocks, and also within the blocks due to the heterogeneity.

\citet{dawe_2008} conducted experiments to investigate cross-flow, and as previously mentioned, the permeability field for the experimental work were somewhat replicated here for the simulations. Therefore, the work of \citet{dawe_2008} can be used to validate the resulting simulations from this work. In particular, it will be noted that the simulations matches the experiment at the boundary between $K1$ and $K2$. Additionally, the cross-flow from $K1$ to $K4$ is well represented in the simulations.

Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} show snapshots of phase 1 (\ie injected or displacing fluid) saturation field for all studied cases. In Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation}(a) and (b), even though fluid is injected at the same rate across the left face it travels faster in $K1$ compared to $K2$. Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} (c) show that fluid experience a resistance to flow at the interface between $K1$ and $K3$ which encourages the flow into $K4$ (Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} (d) and (e)). Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} (f) indicates preferential flow in $K4$ compared to $K3$.

Figure~\ref{fig:Saturationfield@t=1.15s} compares phase 1 saturation distribution for all the cases at $t=1.15s$. Generally, saturation distribution for all cases closely resemble each other with arithmetic and harmonic mean (Figs.~\ref{fig:Saturationfield@t=1.15s}(b) and (c)) being the most identical. Albeit, closer inspection will reveal that there is more displacement in Fig.~\ref{fig:Saturationfield@t=1.15s}(b) compared to (c). This is logical giving that the block mean values calculated for arithmetic mean is higher than that for harmonic mean. However, the important feature of Figs.~\ref{fig:Saturationfield@t=1.15s}(b) and (c) is that they poorly represent the BaseCase (Fig.~\ref{fig:Saturationfield@t=1.15s}(a)) at the top left hand corner of block $K2$.

Figure~\ref{fig:Saturationfield@t=1.15s}(d) also shows an inconsistency with the BaseCase (Fig.~\ref{fig:Saturationfield@t=1.15s}(a)) where it shows phase 1 moving from block $K2$ into $K4$. This inconsistency is with respect to this realization of the PDFCase. Hence, PCA-LinReg Case in Fig.~\ref{fig:Saturationfield@t=1.15s}(e) give the best representation of the BaseCase in \ie Fig.~\ref{fig:Saturationfield@t=1.15s}(a)

\section{Conclusion}\label{section:conclusion}

Upscaling is a form of reduced order model, and a method of implementing upscaling by coupling linear regression with PCA is introduced in this work. For a 2-D model, SVD is used to decompose the existing permeability field to obtain the PCA space, while HOSVD is used for 3-D models. Within, the PCA space, linear regression (or interpolation) is used to reduce the data in the PCA space. The reduced data in the principal component space is then recombined to form a reduced data of the permeability field.

Results show that the permeability field data reduction can be specifically defined for the PDFCase and the PCA-LinReg Case but not for the ArithMeanCase and the HarmMeanCase. This is because the latter cases will produce the same permeability field irrespective of the upgrid resolution. The PCA-LinReg Case is the only technique that does not require a prior knowledge of blocks with similar permeability values before upscaling. This is a huge advantage as it saves time during the data pre-processing stage, because sorting and calculations for each distinct region is avoided. 

The author's future work will provide a quantitative analysis of the results obtained from the coupled PCA-Linear regression method. Additionally, future work will present other methods which follow a similar principle but also seek to optimize the quantitative results obtained.


%\section{Acknowledgements}
%Mr Babatunde Lashore 

\clearpage 
%% References with bibTeX database:
\bibliographystyle{elsarticle-harv} 
%\bibliographystyle{elsarticle-num}
%\bibliographystyle{apacite}
\bibliography{references}
  
\clearpage 

%\listoftables
%\clearpage
%\input{article_table}
\clearpage  
\listoffigures
\clearpage
\input{article_figure1} 

\clearpage
\appendix
\section{\\A Brief Introduction to SVD}\label{subsection:svd_brief}
Given a matrix $\mathbf{A}\in\mathcal{R}^{m\times n}$, the singular value decomposition (SVD) is a method for factorizing $\mathbf{A}$, such that,
\begin{equation}
  \mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\intercal}.
\end{equation}
where $\mathbf{U}=\left[\underline{u}_{1},\underline{u}_{2},\cdots,\underline{u}_{m}\right]\in\mathcal{R}^{m\times m}$ and $\mathbf{V}=\left[\underline{v}_{1},\underline{v}_{2},\cdots,\underline{v}_{n}\right]\in\mathcal{R}^{n\times n}$ are orthogonal but not necessarily the same. $\Sigma$ is a diagonal matrix, \ie
\begin{displaymath}
  \Sigma=\text{diag}\left(\sigma_{1},\sigma_{2},\cdots,\sigma_{r}\right)\in\mathcal{R}^{m\times n},
\end{displaymath}
and $r=\min\left\{m,n\right\}$ is the rank of $\mathbf{A}$, with $\sigma_{1}\ge\sigma_{2}\ge\cdots\sigma_{r}\ge 0$. The diagonal entries of $\Sigma$ $\left(\text{\ie} \sigma_{i}, i=1,r\right)$ are called singular values of $\mathbf{A}$. If $\mathbf{A}$ is a square matrix then $\mathbf{U}$, $\Sigma$ and $\mathbf{V}$ are also square matrices. Also, as $\mathbf{U}$ and $\mathbf{V}$ are orthogonals and unitary, 
\begin{equation}
  \mathbf{U}^{\intercal}\mathbf{U} = \mathbf{V}^{\intercal}\mathbf{V} = \mathbf{I},
\end{equation}
where $\mathbf{I}$ is an identity matrix. However, if $\mathbf{A}$ is an $m\times n$ matrix, $\mathbf{U}$ and $\mathbf{V}$ still remain as square matrices, albeit with different matricial sizes. $\mathbf{U}$ is an $m\times m$ while $\mathbf{V}$ is an $n\times n$. Another relevant property of $\mathbf{U}$ and $\mathbf{V}$ since they are square (irrespective of whether $\mathbf{A}$ is square or rectangular) is that
\begin{equation}
  \mathbf{U}^{-1} = \mathbf{U}^{\intercal} \;\;\;\text{ and }\;\;\; \mathbf{V}^{-1} = \mathbf{V}^{\intercal},
\end{equation}
and if $\mathbf{A}$ is an $m \times n$ matrix thus $\Sigma$ is a diagonal $m \times n$ matrix.

\medskip
For a rectangular matrix, the singular values fill the first $r$ places on the main diagonal of $\Sigma$. The SVD benefits from the orthogonality of the columns in $\mathbf{U}$ and $\mathbf{V}$. The columns of $\mathbf{U}$ are known as the left singular vectors while the columns in $\mathbf{V}$ are known as the right singular vectors. A simple consideration of the benefit of the orthogonal factors, $\mathbf{U}$ and $\mathbf{V}$, is the ease with which they allow the inverse of the square matrix $\mathbf{A}$ to be calculated. This in turn makes it easy to solve the linear equations, $\mathbf{A}x = b$ with the SVD factorization,
\begin{equation}\label{e:SVD_factorization}
 \mathbf{U} \Sigma \mathbf{V}^{\intercal} x = b \;\;\Longrightarrow\;\; x = \mathbf{V} \Sigma^{-1} \mathbf{U}^{\intercal} b,
\end{equation} 
with benefit of the orthogonal property already discussed.


\section{\\A Brief introduction to HOSVD}\label{subsection:hosvd_brief}
Using the tensor notation from \citet{Kolda_2009}, given a tensor $\mathscr{X}\in\mathcal{R}^{N_{1}\times N_{2}\times \cdots \times N_{m}}$, the higher order singular value decomposition (HOSVD) is a method for factorizing $\mathscr{X}$, such that,
\begin{equation}
  \mathscr{X} = \mathscr{G} \times_{1} \mathbf{U}^{1}\times_{2} \mathbf{U}^{2}\times \cdots \times_{m} \mathbf{U}^{m}.
\end{equation}
where tensor $\mathscr{G}\in\mathcal{R}^{N_{1}\times N_{2}\times \cdots \times N_{m}}$ and is normally referred to the core tensor. Sub-tensors of $\mathscr{G}$ are orthogonal and ordered.\\
And matrices $\mathbf{U}^{m}\in\mathcal{R}^{N_{m}\times N_{m}}$ are orthogonal factor matrices which could be thought of as the principal components in each mode


\end{document}
%% End of tex file.


