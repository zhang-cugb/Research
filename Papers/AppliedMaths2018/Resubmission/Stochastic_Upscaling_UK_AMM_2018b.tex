
% Copyright 2007, 2008, 2009 Elsevier Ltd 
% 
% This file is part of the 'Elsarticle Bundle'.
% --------------------------------------------- 
%
% It may be distributed under the conditions of the LaTeX Project Public
% License, either version 1.2 of this license or (at your option) any
% later version.  The latest version of this license is in
%    http://www.latex-project.org/lppl.txt
% and version 1.2 or later is part of all distributions of LaTeX
% version 1999/12/01 or later c.
%
% The list of all files belonging to the 'Elsarticle Bundle' is
% given in the file `manifest.txt'.
%
 
% Template article for Elsevier's document class `elsarticle'
% with harvard style bibliographic references
% SP 2008/03/01
%
%
%
% $Id: elsarticle-template-harv.tex 4 2009-10-24 08:22:58Z rishi $
%
%
%\documentclass[preprint,authoryear,12pt]{elsarticle}
\documentclass[preprint,12pt]{elsarticle}

% Use the option review to obtain double line spacing
%\documentclass[authoryear,preprint,review,12pt]{elsarticle}

% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
% for a journal layout:
%\documentclass[final,authoryear,1p,times]{elsarticle}
%\documentclass[final,authoryear,1p,times,twocolumn]{elsarticle}
%\documentclass[final,authoryear,3p,times]{elsarticle}
%\documentclass[final,authoryear,3p,times,twocolumn]{elsarticle}
%\documentclass[final,authoryear,5p,times]{elsarticle}
%\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the gra

%%graphis package for simple commands
%% \usepackage{graphics}
%\usepackage{cases}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
\usepackage{epsfig}
%\usepackage{subfig}
\usepackage{comment}

\usepackage{epstopdf}
\usepackage{pdflscape}
\usepackage{bm}
\usepackage{hyperref,url}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=red}

%% The amssymb package provides various useful mathematical symbols

\usepackage{amssymb,amsmath,array}
%The amsthm package provides extended theorem environments

\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[mathscr]{euscript}
%\usepackage{subfigure}
  
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

\usepackage{lscape}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\newcommand{\JGnote}[1]{\fbox{\parbox{\textwidth}{ \color{red} JG Note $\Rightarrow$ #1}}}
\newcommand{\BLnote}[1]{\fbox{\parbox{\textwidth}{ \color{green} BL Note $\Rightarrow$ #1}}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\green}{\textcolor{green}}
\newcommand{\yellow}{\textcolor{yellow}}
\newcommand{\frc}{\displaystyle\frac}
\newcommand{\PN}[2][error]{P$_{#1}$DG-P$_{#2}$}
\newcommand{\PNDG}[2][error]{P$_{#1}$DG-P$_{#2}$DG}
\newcommand{\eg}{{\it e.g., }} 
\newcommand{\ie}{{\it i.e., }}
\newcommand{\st}{{\it s.t., }}

\journal{Applied Mathematical Modelling}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}} 
%% \fntext[label3]{}

%\title{A Reduced Order Model for Porous Media Flows which Couples Linear Regression with Principal Component Analysis (PCA) using SVD or HOSVD: Upscaling the Permeability Field for Multiphase Reservoir Flow Simulation}
\title{A Reduced Order Model for Porous Media Flows: Coupling Linear Regression with Principal Component Analysis (PCA) using SVD or High-Order Singular Value Decomposition (HOSVD) in Upscaling Calculations for Permeability in Reservoir Simulations}
\author[UoA]{B. Lashore\corref{cor1}}\ead{lashorebabatunde@yahoo.com} \author[UoA]{K. Christou} \author[UoA]{J.L.M.A. Gomes}
\cortext[cor1]{Corresponding author.}
\address[UoA]{Mechanics of Fluids, Soils \& Structures Research Group \\ School of Engineering, University of Aberdeen, UK}


\begin{abstract}

Representation of heterogeneous properties (\ie permeability in this case) within a large system, is a challenge that cuts across many industries that deal with flows in porous media. This is because it is impossible to obtain the value of permeability at every point in realistic domains. And, even if this was possible, it would be extremely challenging to get the computing resources to make use of all available data values. This challenge gave birth to upscaling, a model order reduction method.

In this paper, a single realization of a permeability field was upscaled by first projecting the permeability field into a principal component analysis space (using SVD for 2D field and HOSVD for 3D fields). Then linear regression was employed within each of the principal component spaces to obtain approximate representations of the initial principal component. The new principal components are then recombined to for a new permeability field with a reduced order. This PCA-linear regression method was compared to traditional industry-standard upscaling techniques (\eg arithmetic and harmonic averaging) and to a stochastic-based (probability density function, PDF) method of upscaling, to highlight its benefits/performance.

It was shown that the new upscaling technique retained the heterogenous nature of the BaseCase (\ie high-resolution configuration). Additionally, the upscaling technique did not require {\it a priori} understanding of similar permeability blocks within the existing domain.  \red{Later}

\end{abstract}



\begin{keyword} %% keywords here, in the form: keyword \sep keyword
Principal Component Analysis (PCA) \sep Linear Regression \sep Higher-Order Singular Value Decomposition (HOSVD) \sep Upscaling \sep Reduced Order Models (ROM) \sep Permeability.  
\end{keyword}
 
\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Introduction}\label{section:intro}
Naturally occuring rock formations are inherently heterogeneous, which means that geophysical properties, such as permeability, pore spaces (\ie porosity) and pore throat vary spatially at different length scales. Heterogeneity of rock formations is relevant in calculations of transport and storage of fluids in porous media (\ie multiphase flow in subsurface rocks). Therefore, the representation of such heterogeneous properties is of great importance to oil and gas (\ie reservoir management), carbon capture and storage (CCS, \ie CO$_2$ transportation and storage in subsurface rocks) and waste management (\ie remediation of contaminated soil).

In essence, rock heterogeneity makes it impossible to obtain reliable and accurate spatial information about geological properties throughout the domain (\ie uncertainty problem). Additionally, due to limitations in computing resources, it is neither efficient nor possible to use exact values of geological properties at all spatial coordinates of the domain to perform fine-grid simulations (\ie scaling problem) \cite{chen_2006,miller_1998,Renard_1997}. This is particularly true for large geological domains which span several kilometres. It is possible to overcome these challenges by a family of techniques called {\it upscaling}. Upscaling techniques solve both uncertainty and scaling problems by replacing discrete geological and fluid properties of detailed high-resolution domains with coarse descriptions (\ie low-resolution) of these properties \cite{Vereecken_2007}.

Upscaling techniques which best preserve statistical geophysical and thermo-physical properties of high-resolution domains result in optimal prediction of fluid flow dynamics. Over the past 60 years, several upscaling techniques were developed (see \cite{Hasting_2001, Renard_1997, Szymkiewicz_2013}). Traditional upscaling techniques are deterministic in nature, however over the last decade, techniques classed as stochastic have received great attention from academic and industrial porous media communities worldwide \cite{Guilleminot_2012, Ravalec-Dupin_2010, Verwoerd_2009}. One of the reasons for this is because stochastic upscaling techniques are robust enough to handle multiphase flow in porous media and they are able to address scaling and uncertainity problems.statistical

\medskip
{\it Upscaling} is a term commonly used in the oil and gas sector and is often referred, throughout applied mathematics and engineering communities, as a form of model order reduction (or reduced order models, MOR/ROM). Depending on the context in which MOR is used, it can be simply referred as `dimensionality reduction'. The definition of MOR also depends on the context in which it is used, but it suffices to state that MOR's are ale to captures essential features of a system, structure or parameter \cite{Schilders2008}. This implies that an original system exists with more detailed information. Figure~\ref{fig:IllustrationMOR} presents an interpretation of MOR concept in which a graphical representation of a rabbit is reconstructed with a reduced number of facets (rhs of this illustration).  

\citet{Schilders2008} provided a historical perspective on the development of Model Order Reduction, from the work of Fourier in 1807 which approximated a function with a few trigonometric terms, to the work of \citet{Odabasioglu_1998} in 1998, which introduced the passive reduced-order interconnect macromodeling algorithm (PRIMA). Truncated balanced realization is another MOR method developed by \citeauthor{Moore_1981}~\cite{Moore_1981}, which introduced the principal component analysis (PCA) \cite{Hotelling_1933} and \citeauthor{Golub1970}'s \cite{Golub1970} algorithm for solving singular value decomposition (SVD). Hankel-norm reduction \cite{Glover_1984} and proper orthogonal decompositions (POD) \cite{Sirovich_1987} are also MOR methods discussed in the references published in 1984 and 1987, respectively. The asymptotic waveform evaluation (AWE) \cite{Pillage_1990} was the first MOR method based on Krylov subspace technique, this method was followed by Pade-via-Lanczos method \cite{Feldmann_1995} and the PRIMA.

\bigskip
 In this work, upscaling is performed through linear regression in the PCA space which is obtained with high-order singular value decomposition (HOSVD/SVD) in 2-/3-D problems. This work focuses on reducing the order of permeability distribution for porous media flow simulations. 

\bigskip
A brief overview of current upscaling methods in reservoir simulation is described in Section~\ref{section:general_upscaling}. Section~\ref{section:mathematical_model} introduces the mathematical model used to describe the relevant physics for multi-fluid flow through porous media and highlights the relevance of the permeability field. The mathematical model is discretised by a high-order accurate control volume finite element method (CVFEM) \cite{Gomes_2017}. Pre-processing statistical properties and post-simulation multiphase flow behaviour are investigated to assess the various upscaling techniques used. Four upscaling techniques are investigated in this work and they are named arithmetric mean, harmonic mean, probability density function (PDF) and PCA (using SVD and HOSVD methods) coupled with linear regression. In Section~\ref{section:pca_linreg}, fundamental mathematics of SVD and associated properties are presented. Then, the concept of spaces is explained to give the reader the understanding necessary to appreciate the transformations and projections involved in this work. Finally, Section~\ref{section:pca_linreg} describes the reduced order model reduction for the permeability field within the principal component space. Section~\ref{section:model_simulation} is dedicated to model description and simulation. It starts by describing the high-resolution problem (\ie `base case') in which the four upscaling techniques are applied to. A brief summary of the pre-processing steps for each of the models is also presented. Section~\ref{section:results_discussion} provides the results and further discussion of the results, it also gives further interpretation to the result obtained from the SVD upscaling technique. Finally, conclusions are drawn in Section~\ref{section:conclusion}.


\section{Upscaling}\label{section:general_upscaling}
\subsection{Brief Overview of Upscaling}\label{section:overview_upscaling}

Upscaling methods replace discrete geological and fluid properties of detailed (high-resolution) domains with coarse descriptions (low-resolution) of these properties \cite{Vereecken_2007}. Upscaling of heterogeneous properties has attracted the interest of several porous media flow communities (\eg oil and gas, CCS and waste management) worldwide for over $7$ decades. Those interested in large-scale reservoirs or groundwater generally take a two-stage geo-statistical approach. 

In the first stage, information from seismic data, well data and analogous outcrops is used to model the large-scale heterogeneities associated with facies. In the second stage, statistical models such as continuous multi-variate Gaussian field is used to model rock properties of the facies \cite{Ewing_1997}. In addition to seismic and well data used in the previous stage, core data can be used as a source of information for the statistical properties used to determine random fields in this stage.

A number of studies were carried out on upscaling of subsurface permeability (see \cite{Christie_2001SPE10Model, Ewing_1997, Hasting_2001, Indelman_1993, King1996, Vereecken_2007, Wen_1996, Yeo2001} for a few key references). \citet{Cardwell_1945} investigated arithmetic and weighted averages as upscaling techniques for calculating a single equivalent permeability for oil reservoirs with varying permeabilities. The general conclusion reached from this study was that the equivalent permeability lies between harmonic and arithmetic averages.  

\citet{Yeo2001} discussed the accuracy of renormalization method for upscaling 2D hydraulic conductivity fields for two cases. Both cases employed the $4$-quadrant model (see Section~\ref{subsection:model} for details) set-up. The first case had low permeability in all blocks except the bottom right block which had high permeability, while the second case employed the check-board style described in Section~\ref{subsection:model}. Different conductivity ratios were also investigated for the two cases. The conclusion drawn was that renormalization worked fairly well for the first case, but very poorly for the second case.

\citet{Renard_1997} provided a comprehensive review of the various techniques for upscaling. They found out that the techniques classed broadly under deterministic, stochastic and heuristic complemented each other rather than antagonising each other. They also recommended to select a technique which provides block permeabilities instead of one which provides uniform effective permeability. Finally, \citet{Christie_2001SPE10Model} provided $2$ sets of problems to compare the upscaling and upgridding (\ie mesh coarsening) techniques of different simulators. The paper does not describe in any detail the upscaling techniques used by the different simulators, but it provides a complete dataset for flow simulation models' benchmarking.


\subsection{PCA, SVD, POD, KLT and Reservoir Characterisation}\label{section:reservoir_char}

PCA, SVD and POD are closely related. In the petroleum engineering community where these techniques are used for reservoir characterization, they are sometimes used interchangable, and are normally referred to as Karhunen-Loeve Transformations (KLT) \cite{Jafarpour_2009}.

A number of recent research in reservoir engineering focus on  history matching (HM) problems (see \cite{Afra_2013, Insuasty_2017, Jafarpour_2009, Tavakoli_2010, Xiao_2018}). History matching is the process of building or adjusting one or more sets of reservoir configurations (or realisations), such that reservoir models are able to closely reproduces past flow behaviour (\eg flow rate, production volume, pressure etc) in a geological formation, aiming to predict future flow behaviour with good accuracy \cite{Rwechungura_2011}. It is important to note that reservoir configurations could be either a numerical model representing the system of parameters (\ie{variables}) which characterise the reservoir, or simply a model for parameterizing one of more components (\ie{variables}) of the system.

\citet{Jafarpour_2009} compares KLT and discrete cosine transform (DCT) and concluded that the DCT is computationally more efficient than the KLT while being almost as accurate. \citet{Tavakoli_2010} noted that HM calculation of sensitivities for all production data with respect to system parameters is not feasible. Hence, they explored a new parameterization for reducing the number of variables based on the principal right singular vectors of the dimensionless sensitivity matrix.

Perhaps, work performed by \citet{Afra_2013,Afra_2016,Afra_2014} and \citet{Insuasty_2017} are more closely related to the one performed here. However, similar to the work of \citet{Jafarpour_2009}, they are more related to comparing several realizations than reducing the dimensionality of the dataset for a single realization. The work of \citet{Xiao_2018} is also closely related to that performed here and interestingly, they used the same flow simulator model used in this work. However, similarly to \citet{Tavakoli_2010}, \citeauthor{Xiao_2018} focused on reducing the number of variable used to describe the system.

Essentially, recent developments in the field of reservoir simulation described in this section make use of SVD theory to select the most suitable realization from multiple realizations or to reduce the representative system variables. However, the work described here is concerned with reducing dimensionality of the dataset for a single realization of a single system variable (\ie{permeability}).


\section{Multi-Fluid Flow Model}\label{section:mathematical_model}

In this work, a control volume finite element method (CVFEM) formulation is used to discretise and solve the set of multi-fluid flow equations. Continuity equations are embedded into the pressure equation to enforce mass conservation and the exact force balance (extended Darcy equations). The numerical formulation employs an implicit algorithm with respect to time which is less restrictive than the traditional industry-standard implicit-pressure-explicit-saturation (IMPES) scheme \cite{aziz_1986,Chen_2007}.

The formulation used in this work uses a dual consistent CVFEM representation which is embedded in novel families of triangular and tetrahedral finite element-pairs, \PN[n]{m} and \PNDG[n]{m}. In this element-pairs velocity is represented by $n^{\rm th}$-order polynomials that are discontinuous across elements, while pressure is represented by $m^{\rm th}$-order polynomials that may be either continuous (\PN[n]{m}) or discontinuous (\PNDG[n]{m}) across elements. Most properties of \PNDG[n]{m} element-pairs are similar to those in the \PN[n]{m} element-pairs but allows a representation in which pressure, saturation and other solution variables (\eg temperature, concentrations etc) are discontinuous across finite element boundaries \cite{adam_2016, salinas_2018}.

Immiscible multi-fluid flows in porous media may be represented by the extended Darcy equation,
\begin{equation}\label{e:darcy_eqn}
  \mathbf{q}_{\alpha} = \mathbf{u}_{\alpha}S_\alpha=
  -\frac{\mathcal{K}_{{r}_\alpha}\mathbf{K}}{\mu_{\alpha}}\left(
  \nabla p_{\alpha} - {\mathbf{s}_{u}}_{\alpha} \right),
\end{equation}
where $\alpha$ designates a phase, $\mathbf{q}_{\alpha}$ is the $\alpha$-th phase Darcy velocity and $\mathbf{u}_{\alpha}$ is the saturation-weighted Darcy velocity. $\mu_{\alpha}$, $p_{\alpha}$, $\rho_{\alpha}$, and $\mathbf{s}_{{u}_\alpha}$ are phase dynamic viscosity, pressure, density and source term (which may include gravity and/or capillarity), respectively. $\mathbf{K}\left(\mathbf{r}\right)$ and $\mathcal{K}_{{r}_\alpha}\left(S_{\alpha}\right)$ are absolute and phase-relative permeability, respectively. The later is a function of the phase saturation $S_{\alpha}\left(\mathbf{r},t\right)$, which in turn is spatial- and time-dependent.

In addition to the extended Darcy equation (Eqn.~\ref{e:darcy_eqn}), saturation conservative equations,
\begin{equation}
  \phi\displaystyle\frac{\partial S_{\alpha} }{\partial t} + \nabla
  \cdot \left( {\mathbf u}_{\alpha} S_{\alpha}\right) =
  s_{cty,\alpha},
  \label{saturation_equation}
\end{equation}
are discretised in space with CV basis functions, and in time with the $\theta$-method \cite{gomes_book_2012}. In Eqn.~\ref{saturation_equation}, $\phi$ is the porosity and $s_{cty}$ is a source term.

\medskip
The discretised saturation and Darcy equations are solved using a multigrid-like approach as described by \citeauthor{pavlidis2016}~\cite{pavlidis2016}. The numerical formulation is fully described by \citeauthor{Gomes_2017}~\cite{Gomes_2017} (see also \cite{adam_2016, salinas2015}). These numerical methods are embedded in the next-generation flow simulator Fluidity/IC-FERST model software\footnote{\href{http://multifluids.github.io}{http://multifluids.github.io}} (a full description of the model can be found in \cite{Gomes_2017, jackson_2013,salinas_2018}). Although, the numerical formulation used here is relatively new, it is not the focus of this work. The result and discussion section (Section \ref{section:results_discussion}) refers to \citet{dawe_2008} for validation of the numerical formulation with respect to the 2D simulation. However, extensive model and software quality assurance (\ie verification and validation tests) can be found in the aforementioned references \cite{adam_2016, Gomes_2017,jackson_2013, pavlidis2016, salinas2015}.


\medskip
Finally, $\mathbf{K}\left(\mathbf{r}\right)$, the focus of this work, is prescribed in the system of governing conservative equations, \ie it is an independent input variable. Since $\mathbf{K}$ is a petrophysical property of geological formations that varies spatially, it cannot be determined at every grid point (\ie spatial coordinate). And, even if this information is available at every spatial coordinate, it would require immense computer resources to process during flow simulations. This is the reason upscaling is required for processing permeability spatial variability. Upscaling can be performed from the continuum scale, across the micro-scale ($mm$), local- and meso-scale ($m$) to field-scale ($km$)\cite{ECMI_2004}. The upscaling discussed here is for micro-scale, also known as the Darcy scale.


\section{Coupled PCA and Linear Regression}\label{section:pca_linreg}

\subsection{Principal Component Spaces and Model Order Reduction for the permeability field}\label{subsection:pcspaces}
  
\subsubsection{Understanding the permeability space}\label{subsubsection:visualization_permspace}
Permeability can be represented as either tensor or scalar quantity, however for the purpose of the following explanation (and the remaining of this work), permeability is assumed as a scalar quantity. In addition, in the numerical simulations performed for this work, permeability is discretised element-wise with projection to the control volume space within each element-pairs (see \cite{Christou_2018}). Permeability values are assigned to the centre of the FE $\left(\text{\ie P}_{0}\text{DG element-pair}\right)$. For simplicity, in the remaining part of this sub-section, a $2D$ geometry is assumed which is discretized with triangular elements (\PN[1]{2} and \PNDG[1]{1} element-pairs). This explanation allows one to easily visualize this permeability space because it is defined by Cartesian coordinate system.

The ROM method discussed here is concerned with reducing the volume of permeability data within the permeability space. Consider for instance a small square which is discretized to contain four squares such that $4$ data points are defined in the middle of the four squares. The permeability space of $4$ can be reduced to a smaller value of $1$ through an averaging process such as arithmetic, harmonic or geometric mean. This type of reduction is the focus of this paper, albeit the main focus of this paper is achieving this reduction through linear regression in the principal component spaces. 

\subsubsection{Understanding the principal component spaces}\label{subsubsection:visualization_pcspaces}
Essentially, the existing permeability space is transformed (with SVD and HOSVD for 2- and 3-D permeability data, respectively) to principal component spaces (PCS). Within the PCS, traditional PCA dimensionality reduction can be optionally performed (\ie{feature selection or elimination}, Section~\ref{subsubsection:pca_dimred_linreg}), before interpolation is used to implement further data reduction in PCS. The reduced data in the PCS is then transformed back into the permeability space to obtain a reduced permeability space. An algorithm which impliments the procedure described here is presented in Section~\ref{subsubsection:svdcase_preprocess_algorithm}

The PCS is a particularly challenging to visualize as it represents projections which are embedded in SVD/HOSVD factors of the original permeability dataset. An aid for visualizing the PCS for a 2D permeability data is to write out $\mathbf{U}$, $\mathbf{\Sigma}$ and $\mathbf{V^{\intercal}}$ matrices (or SVD factors) and consider the singular vectors (\ie columns in $\mathbf{U}$ and rows in $\mathbf{V^{\intercal}}$) as separate PCS. A similar procedure can be extended to HOSVD for 3D permeability dataset. The decomposition of a matrix into its factors using SVD \cite{Cline_2007, Tharwat_2016} or HOSVD \cite{Kolda_2009, Lathauwer_2000} is covered in details in several published materials. However, for completeness, \ref{subsection:svd_brief} and \ref{subsection:hosvd_brief} contain brief overviews of SVD and HOSVD, respectively.

\subsubsection{Reduced Order Model/Principal Component Space Reduction}\label{subsubsection:pca_dimred_linreg}
For the PCA-Linear regression reduced order model (PCA-LinReg) of the permeability data, all work is performed in PCS. This can be conducted in two steps (for 2-D models), thus:
\begin{description}
  \item[Step 1:] Perform PCA \cite{Hotelling_1933, Tharwat_2016} using the SVD method, where the singular values are examined and, based on a pre-defined criteria, $z$ singular values are selected, starting from $\sigma_{1}$ and in a decreasing order, where $z \leq r$. Note that, in the case with $z = r$, it can be assumed that {\it Step 1} has been ignored. However, when $z < r$, the first $z$ columns of $\mathbf{U}$ and the first $z$ rows of $\mathbf{V^{\intercal}}$ are extracted from $\mathbf{U}$ and $\mathbf{V^{\intercal}}$ to form new matrices $\mathbf{U_{new}}$ and $\mathbf{V^{\intercal}_{new}}$ respectively. The selected $z$ singular values also form a new diagonal matrix, $\mathbf{\Sigma_{new}}$ .

  \item[Step 2:] Interpolation within each singular vector (column in $\mathbf{U}$ and row in $\mathbf{V^{\intercal}}$) is conducted to reduce the number of data points in each column of $\mathbf{U}$ and in $\mathbf{V^{\intercal}}$. From this step,  $\mathbf{U_{new2}}$ and $\mathbf{V^{\intercal}_{new2}}$ are obtained. The reduced permeability field is calculated by multiplying  $\mathbf{U_{new2}}$, $\mathbf{\Sigma_{new}}$ and $\mathbf{V^{\intercal}_{new2}}$.

\end{description}

These two steps can be readily replicated for 3-D models using HOSVD to obtain the PCA space.

Finally on the PCS, it is worth noting that two methods exist for obtaining PCA space: co-variance and SVD methods (see \cite{Tharwat_2016} detailed description of both methods). In this work, the authors have opted to use the SVD method to obtain the PCA space, as singular vectors which make up the PCS have strong correlation to the coordinate axis they represent. This was exploited in making the assumption that a linear relationship exist for the data in each of the PC spaces, hence the linear regression performed to reduce the data in the PCS. 

\subsection{Principal Component Analysis Versus Linear Regression}\label{subsection:pcs_vs_linreg}

\citet{AndrewNg_2018} was quoted to have said ``PCA is not linear regression, and despite some cosmetic similarity, these are actually totally different algorithms''. The main differences between these two mathematical methods is summarised in the Table~\ref{table:pca_vs_linreg} and can also be deduced from Figure~\ref{fig:PCA_is_not_LinReg}.

\begin{table}[h!]
\centering
\begin{tabular}{|m{1em} |m{14em} |m{14em}|} 
 \hline
  & \begin{center}{\bf PCA}\end{center} & \begin{center}{\bf Linear Regression}\end{center} \\ [0.5ex] 
 \hline
 1 & Determines the best vector (or set of vectors) which compresses the data points by minimizing the orthogonal distance to the points & Approximates the best fit for the points by minimizing the error \\
 \hline
 2 & Essentially, it forms a projection of the original points onto the new vector space & Predicts $y$ from $x$ through a function $f(x_1,x_2,....,x_n) = y$ \\
 \hline
 3 & Not used for predictions, as original feature space (\ie $x_1$ and $x_2$) give way to a new reduced dimension. Cordinates $x_1, x_2,...,x_n$ give way to $z_1,....,z_k$ where $k \leq n$ & \\[1ex]
 \hline
\end{tabular}
\caption{Table comparing PCA with Linear Regression.}
\label{table:pca_vs_linreg}
\end{table}


\section{Model Description and Simulation Setup}\label{section:model_simulation}

As previosly mentioned, four upscaling techniques are investigated in this work. Arithmetic and harmonic means are used to obtain the first two upscaled representation and these two are classed as deterministic techniques. The third is a randomly generated permeability field with Gaussian distribution, prescribed by a probability density function (PDF) which was obtained from a domain discretised with high-resolution mesh with known permeability distribution (\ie `base case'). The fourth upscaling technique is the crux of this research, it introduces a reduced order model which couples PCA and linear regression.

\subsection{Model Description}\label{subsection:model}
The BaseCase  (\ie high-resolution mesh) configuration for the permeability dataset builds on the $2 \times 2$ block, \ie $4$ quadrant model orginally used by \citet{Cardwell_1945} and later by \citet{Yeo2001} and \citet{dawe_2008}. In the domain designed by these authors, permeability is represented as checker-board in which diagonally opposite blocks retained the same permeability values with one set of diagonally opposite blocks having a higher permeability value compared to the other set. Hence, one set of diagonally opposite blocks are referred to as high permeability blocks while the other set is referred to as low permeability blocks.

The BaseCase permeability model used here retains the checkerboard desgined used by these references. Albeit, the permeability dataset for each block in the set of diagonally opposite blocks have similar value (\ie not exactly the same as in the papers referred to). %, although one set can still be referred to as a set of high permeability blocks while the other set is referred to as a set of low permeability blocks.
Furthermore, within each block, permeability dataset varies within a pre-defined range and consists of $1600$ permeability values. Details on the design of the permeability field for the BaseCase are described in Section~\ref{subsubsection:basecase_preprocess_algorithm}. Figures~\ref{fig:PermFields}a-b show permeability distribution for the BaseCase with the associated mesh resolution.  Using the labelling scheme used by \citet{dawe_2008}, the quadrant model block $K1,~K2,~K3$ and $K4$ are top left, bottom left, top right and bottom right, respectively.

As previously mentioned, four upscaled techniques applied to permeability dataset were obtained from the high-resolution (\ie the BaseCase) configuration. Prior to upscaling, mesh resolution of the BaseCase configuration was coarsened, Fig.~\ref{fig:HiRes_LowRes_Mesh}, in which the number of triangular elements was reduced from 4112 to 728. Arithmetic (ArithMean) and harmonic (HarmMean) averaging methods were initially applied to the BaseCase configuration. In both sets of test-cases, averages are calculated for each block using the $1600$ data points from the blocks of the BaseCase to obtain permeability values for each block in the upscaled models. This effectively homogenized the permeability dataset in each block. Permeability dataset was also upscaled using probability density function (PDFCase) and SVD (PCA-LinReg Case). Description of these models are introduced in Sections~\ref{subsubsection:pdfcase_preprocess_algorithm}-\ref{subsubsection:svdcase_preprocess_algorithm}. Different from ArithMean and HarmMeanand, PDFCase and PCA-LinReg Case are able to retain heterogeneity within each block.

\subsubsection{BaseCase Pre-processing Algorithm}\label{subsubsection:basecase_preprocess_algorithm}
\begin{description}
  \item[Step 1:] A function is used to randomly generate uniform (\ie Gaussian) dataset for each block. The range for each block is as below:
  \begin{enumerate}[a)]
    \item \textbf{K1} : $700 - 900$ mD;
    \item \textbf{K2} : $50 - 150$ mD;
    \item \textbf{K3} : $100 - 200$ mD;
    \item \textbf{K4} : $1000 - 1200$ mD; 
  \end{enumerate}                                                    
  \item[Step 2:] The number of data points generated for each block is $1600$, and data is randomly distributed within each block in a structured pattern. Albeit, the mesh is unstructured;
  \item[Step 3:] Merge the dataset from different blocks into a single file retaining the original ``structured pattern'' ({\it Step 1}) for the randomly positioned dataset for each block. This data-structure will be used by the CVFEM-based flow simulator to prescribe the permeability dataset for the BaseCase configuration.
\end{description}

\subsubsection{PDFCase Pre-processing Algorithm}\label{subsubsection:pdfcase_preprocess_algorithm}
\begin{description}
  \item[Step 1:] Datasets from {\it Step 1} of Section~\ref{subsubsection:basecase_preprocess_algorithm} are used to calculate mean and standard deviation for each block of permeability data;
  \item[Step 2:] A function which generates a Gaussian dataset using arithmetic mean and standard deviation is used to stochastically produce a quarter of the data points from the BaseCase configuration for each of the four blocks (\ie $400$ data points for each block);
  \item[Step 3:] Data values in the dataset for each block is randomly positioned within each block in a structured pattern. Similarly to the BaseCase configuration, simulation mesh grid is also unstructured;
  \item[Step 4:] Finally, the structured/arranged dataset from each block is combined into a single data-structure and then used by the flow simulator, which merge the permeability mapping to the mesh with the appropriate element-pairs (in simulations shown here, \PN[1]{2} and \PNDG[1]{1} element-pairs are used).
\end{description}

\subsubsection{2-D PCA (SVD) \& LinReg Case Pre-processing Algorithm}\label{subsubsection:svdcase_preprocess_algorithm}
\begin{description}
  \item[Step 1:] Permeability data obtained in {\it Step 3}  of Section~\ref{subsubsection:basecase_preprocess_algorithm} is factorized using SVD method to obtain $\mathbf{U}, \Sigma,$ and $\mathbf{V}$;
  \item[Step 2:] A selected number ($z$) of singular values are retained in decreasing order of magnitude to form a new diagonal matrix, $\Sigma_{new}$ (this step is optional as it is the conventional PCA dimensionality reduction);
  \item[Step 3:] First $z$ columns of $\mathbf{U}$ and $\mathbf{V}$ are selected to form new truncated matrices called $\mathbf{U_{new}}$ and $\mathbf{V_{new}}$, respectively. This step is necessary only if the optional step in 2 above is executed;
  \item[Step 4:] Within each column of $\mathbf{U_{new}}$ and $\mathbf{V_{new}}$, a linear interpolation is performed to form a new column with the number of data points in each column reduced to half of the original column. These new columns are recombined to form $\mathbf{U_{new2}}$ and $\mathbf{V_{new2}}$;
  \item[Step 5:] Permeability dataset used for the PCA-LinReg Case is the matrix $\mathbf{A_{new2}}$, which is defined as the product of $\mathbf{U_{new2}} \mathbf{\Sigma_{new}} \mathbf{V_{new2}^{\intercal}}$. It is a permeability dataset with a quarter of the data values of the BaseCase, \ie a permeability dataset with reduced order.  
\end{description}


\subsubsection{3-D PCA (\ie{using HOSVD}) \& LinReg Case Pre-processing Algorithm}\label{subsubsection:hosvdcase_preprocess_algorithm}
The algorithm for 3-D case is similar to that of the 2-D case described previously. However:
\begin{enumerate}[1.]
  \item In {\it Step 1} of Section~\ref{subsubsection:svdcase_preprocess_algorithm}, HOSVD method is used for factorization instead of SVD. Therefore, a $3 \times 3$ core tensor is obtained in place of the $\mathbf{\Sigma}$ term, while 3 factor matrices are obtained in place of $\mathbf{U}$ and $\mathbf{V}$;
  \item The core tensor is treated as the $\mathbf{\Sigma}$ term, while the 3 factor matrices are treated similarly to $\mathbf{U}$ and $\mathbf{V}$ as described in Section~\ref{subsubsection:svdcase_preprocess_algorithm}.
\end{enumerate}
The authors found that the Tensorly library package \cite{Tensorly_2018} helpful for implementation of the HOSVD algorithm.

\subsection{Simulations}\label{subsection:simulations}
In all performed numerical simulations, the domain is initially fully saturated with a fluid (Phase 2), and a wetting phase (1) fluid is driven into the domain from the left hand side at a constant mass flow rate. A no-flux boundary conditions were imposed on upper and lower borders of the domain, while mixed fluids left the domain from the right-hand border of the domain. Relative permeability, $\mathcal{K}_{r\alpha}$ (Eqn.~\ref{e:darcy_eqn}), which is often expressed as a function of local, residual and maximum phase saturations, is described in the pore rock matrix by the modified \citet{Brooks_1964} (see \citep{alpak_1999}),
\begin{eqnarray}
  \mathcal{K}_{rw}\left(S_{w}\right) &=& \mathcal{K}^{\circ}_{rw}\left[\frc{S_{w}-S_{w,irr}}{1-S_{w,irr}-S_{nw,r}}\right]^{n_{w}}, \label{Eqn:CoreyBrooks1}\\
  \mathcal{K}_{rnw}\left(S_{nw}\right) &=& \mathcal{K}^{\circ}_{rnw}\left[\frc{S_{nw}-S_{nw,r}}{1-S_{w,irr}-S_{nw,r}}\right]^{n_{nw}}, \label{Eqn:CoreyBrooks2}
\end{eqnarray}
where subscripts $w$ and $nw$ stand for wetting and non-wetting phases, respectively. $\mathcal{K}^{\circ}_{rw}$ and $\mathcal{K}^{\circ}_{rnw}$ are end-point relative permeability to wetting and non-wetting phases, $S_{w,irr}$ and $S_{nw,r}$ are irreducible wetting and residual non-wetting phase saturations, respectively. Exponents $n_{w}$ and $n_{nw}$ are both set to 2.

Figure~\ref{fig:PermFields} shows permeability mapping for all the 2-D cases, whereas Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} show phase saturation distribution during fluid injection for time,
\begin{displaymath}
  t = 0.15,~0.30,~0.50,~1.15,~1.75\text{ and } 2.95 \text{ s for all the 2-D cases.} 
\end{displaymath}

Additionally, the number of \PN[1]{2} triangular element-pairs mesh resolution was reduced from $4112$ (BaseCase, Fig,~\ref{fig:HiRes_LowRes_Mesh}a) to $728$ (upscaled cases, Fig,~\ref{fig:HiRes_LowRes_Mesh}b), a reduction by a factor of approximately $5.6$.

\section{Results and Discussion}\label{section:results_discussion}
After reduction of dimensionality of the permeability dataset using the methods described in Sections~\ref{subsection:pcspaces} and \ref{subsection:model}), numerical simulations were performed with Fluidity/IC-FERST model (see Section~\ref{section:mathematical_model} and \cite{Gomes_2017, jackson_2013,salinas_2018}).

\subsection{2-D Models and Simulations}\label{subsection:results_discussion_2d}
Figure~\ref{fig:PermFields} shows permeability distribution for all the 2-D cases and highlights homogeneous representation resulting from arithmetic and harmonic averaged, Figs.~\ref{fig:PermFields}c-d, whereas PDFCase and PCA-LinReg cases (Figs.~\ref{fig:PermFields}e-f) are inherently heterogeneous. Given that the BaseCase is also heterogeneous, it can be stated that models for the PDFCase and PCA-LinReg Case give a qualitatively better representation of the permeability dataset within the BaseCase. An additional consequence of the homogeneity in the ArithMeanCase and the HarmMeanCase is that the coarsening has no influence on the resulting permeability distribution. However, in both, PDFCase and PCA-LinReg Case, permeability distribution will change as mesh resolution is reduced.

\medskip
With respect to the PDFCase and for this specific resolution, it is important to note that Fig.~\ref{fig:PermFields}(e) is only one realiztion of infinitely many possible representations of the PDFCase. Meanwhile, the PCA-LinReg Case will retain this specific permeability distribution (\ie Fig.~\ref{fig:PermFields}(e)) for such mesh resolution (\ie if linear interpolation is used, but this digresses from the aim of this work and it will be discussed in future publications). Therefore, the permeability value at a specific point in the PDFCase is not directly related to the data value at the nearest point in the BaseCase because the randomly generated PDF dataset is also randomly arranged within each block. However, the data value at a point in the PCA-LinReg Case is directly related to the nearest data values with respect to the same point in the BaseCase, throught the interpolation which takes place in the PCS.

\medskip
Furthermore, prior to the pre-processing required for evaluating the ArithMeanCase, HarmMeanCase and the PDFCase it is necessary to have an understanding of the whole domain and pre-define blocks with similar properties (\ie blocks $K1,~K2,~K3$ and $K4$) in this case. However, with the PCA-LinReg Case, knowledge of blocks with similar properties is not required prior to the upscaling. Albeit, one has to select the upgrid resolution carefully, if the resolution is too large then there is the risk of losing relevant data in the interpolation. The limits of the techniques used for the PCA-LinReg Case will be discussed in future work.

\medskip
Before discuss flow simulation results with permeability distribution resulted from methods described in Sections~\ref{subsection:pcspaces} and \ref{subsection:model}), it is beneficial to briefly consider the Buckley-Leverett flow model and cross-flow behaviour which are used to validate the results from this work. 

\citet{buckley_1942} in 1942 derived the Buckley-Leverett equation, an analytical (albeit simplified) two phase flow equation. The equation determined that a plane of constant saturation progresses uniformly through the domain during the flow simulation. The equation is referred to as ``simplified" because of the assumptions made while deriving it, some of which were ignored in this work. Principally, this work ignores the fact that the equation deals with injection into an homogeneous permeability field while this work deals injection into two blocks ($K1$ and $K2$) with different permeabilities. Additionally, the permeability field in each block is heterogeneous. However, the general validation test or expectation from Buckley-Leverett's influence on this work, is that, the injected fluid saturation in $K1$ and $K2$ is expected to have a piston-shape front within those blocks, subject some inconsistency at the boundary between the two blocks, and also within the blocks due to the heterogeneity.

\citet{dawe_2008} conducted experiments to investigate cross-flow, and as previously mentioned, the permeability field for the experimental work were replicated here for the simulations. Therefore, the work of \citet{dawe_2008} can be used to qualitatively validate the resulting simulations from this work. In particular, it will be noted that the simulations matches the experiment at the boundary between $K1$ and $K2$. Additionally, the cross-flow from $K1$ to $K4$ is well represented in the simulations.

Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} show snapshots of phase 1 (\ie injected or displacing fluid) saturation field for all studied cases. In Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation}a-b, even though fluid is injected at the same rate across the left face it travels faster in $K1$ compared to $K2$. Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation}c show that the fluid experiences a resistance to flow at the interface between $K1$ and $K3$ (due to lower permeability at $K2$ and $K3$) which encourages the flow into $K4$ (\ie bottom-right block, Figs.~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation}d-e). Figures~\ref{fig:BaseCase_Saturation}-\ref{fig:SVDCase_Saturation} (f) indicates preferential flow in $K4$ compared to $K3$.

Figure~\ref{fig:Saturationfield@t=1.15s} compares phase 1 saturation distribution for all the 2-D cases at $t=1.15s$. Generally, saturation distribution for all the 2-D cases closely resemble each other with arithmetic and harmonic mean (Figs.~\ref{fig:Saturationfield@t=1.15s}b-c) being nearly identical. Albeit, closer inspection will reveal that there is more displacement in Fig.~\ref{fig:Saturationfield@t=1.15s}b compared to Fig.~\ref{fig:Saturationfield@t=1.15s}c. This is logical giving that the block mean values calculated for arithmetic mean is higher than that for harmonic mean. However, the important feature of Figs.~\ref{fig:Saturationfield@t=1.15s}b-c is that they poorly represent the BaseCase (Fig.~\ref{fig:Saturationfield@t=1.15s}a) at the top left hand-side corner of block $K2$.

Figure~\ref{fig:Saturationfield@t=1.15s}d also shows an inconsistency with the BaseCase (Fig.~\ref{fig:Saturationfield@t=1.15s}a) where it shows phase 1 moving from block $K2$ into $K4$. This inconsistency is with respect to this realization of the PDFCase. Hence, PCA-LinReg Case in Fig.~\ref{fig:Saturationfield@t=1.15s}e gives the best representation of the BaseCase in \ie Fig.~\ref{fig:Saturationfield@t=1.15s}a.

\subsection{3-D Models and Simulations}\label{subsection:results_discussion_3d}
Numerical simulations on 3D were also performed with the upscaled permeability distribution generated from the method described in Section~\ref{subsubsection:hosvdcase_preprocess_algorithm}. Figures~\ref{fig:HiRes_LowRes_3D_Mesh}a-b present the mesh grid used in the 3-D simulations. Figure~\ref{fig:HiRes_LowRes_3D_Mesh}(a), with 2109 tetrahedral elements, is the high resolution grid and is used to run the simulation for the 3-D base case. While Figure~\ref{fig:HiRes_LowRes_3D_Mesh}(b), with 303 tetrahedral elements, is the low resolution grid used to run the simulation for the 3-D PCA(using HOSVD)-LinReg case. The number of tetrahedral elements for the low grid resolution grid is approximately $7$ times less than that for the high resolution grid.

Figure~\ref{fig:3D_PermFields} shows the permeability distribution for the 3-D cases. The aim here is to show that the PCA-LinReg model, previously obtained with SVD for the 2-D case is obtainable with HOSVD for a 3-D model. Therefore, we only concentrate on one upscaling case for the 3-D model, that is the PCA (using HOSVD) LinReg Model which is in Figure~\ref{fig:3D_PermFields}(b). When this model is compared to the base case, Figure~\ref{fig:3D_PermFields}(a), the same advantages of PCA-LinReg model from the 2-D case are observable. For example:
\begin{enumerate}
\item The model retains heterogeneity within each region
\item A single realization will be obtained for the same upgrid resolution
  \item The correlation to local heterogeniety in the base case is maintained
\end{enumerate}

Figure~\ref{fig:Saturationfield4_3DCases}(a)-(c) and Figure~\ref{fig:Saturationfield4_3DCases}(d)-(f) show the phase 1 saturation distribution for the base case at PCA(using HOSVD)-LinReg case for time,

\begin{displaymath}
  t = 0.18,~0.28\text{ and } 0.42 \text{ s.} 
\end{displaymath}

Figure~\ref{fig:Saturationfield4_3DCases}(a) and (d), which represent the phase 1 saturation distribution for the base case and the 3-D PCA-LinReg case respectively at time $t = 0.18$, retain the mesh grid to give a more details view of the flow pattern. Comparing, Figure~\ref{fig:Saturationfield4_3DCases}(b) and (e) as well as Figure~\ref{fig:Saturationfield4_3DCases}(c) and (f) indicates that the 3-D PCA-LinReg model gives a good representation of the flow pattern. A detailed quantitative analysis of this upscaled model will be presented in the authors' future work.

\subsection{Computational Cost}\label{subsection:comp_effort}

An accurate measure of the CPU cost is not available at this time, however a general note can be made regarding the savings in computational costs with respect to the upgridding resolution.

The 2-D base case which had $4112$ elements was run for about $7$ hours while the upscaled 2-D case were completed under $20$ minutes. For the 3-D case, the base case which had $2109$ elements required about $6$ days to run while the upscaled 3-D case was completed in under $6$ hours.

\section{Conclusion}\label{section:conclusion}

Upscaling is a form of reduced order model. A method of implementing upscaling by coupling linear regression with PCA is introduced in this work. For a 2-D model, SVD is used to decompose the existing permeability field to obtain the PCA space, while HOSVD is used for 3-D models. Within, the PCA space, linear regression (or interpolation) is used to reduce data in PCA space. The reduced data in the PCS is then recombined to form a reduced data of the permeability dataset.

Results show that the permeability field data reduction can be specifically defined for the PDFCase and the PCA-LinReg Case but not for the ArithMeanCase and the HarmMeanCase. This is because the latter cases will produce the same permeability dataset irrespective of the coarser resolution. The PCA-LinReg Case is the only technique that does not require a prior knowledge of blocks with similar permeability distribution before upscaling. This is a huge advantage as it saves time during data pre-processing stages because sorting and calculations for each distinct region is avoided. 

The author's future work will provide a quantitative analysis of the results obtained from the coupled PCA-Linear regression method. Additionally, future work will present other methods which follow a similar principle but also seek to optimize the quantitative results obtained.
 

%\section{Acknowledgements}
%Mr Babatunde Lashore 

\clearpage 
%% References with bibTeX database:
\bibliographystyle{elsarticle-harv} 
%\bibliographystyle{elsarticle-num}
%\bibliographystyle{apacite}
\bibliography{references}
  
\clearpage 

%\listoftables
%\clearpage
%\input{article_table}
\clearpage  
\listoffigures
\clearpage
\input{article_figure1} 

\clearpage
\appendix
\section{\\A Brief Introduction to SVD}\label{subsection:svd_brief}
Given a matrix $\mathbf{A}\in\mathcal{R}^{m\times n}$, the singular value decomposition (SVD) is a method for factorizing $\mathbf{A}$, such that,
\begin{equation}
  \mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\intercal}.
\end{equation}
where $\mathbf{U}=\left[\underline{u}_{1},\underline{u}_{2},\cdots,\underline{u}_{m}\right]\in\mathcal{R}^{m\times m}$ and $\mathbf{V}=\left[\underline{v}_{1},\underline{v}_{2},\cdots,\underline{v}_{n}\right]\in\mathcal{R}^{n\times n}$ are orthogonal but not necessarily the same. $\Sigma$ is a diagonal matrix, \ie
\begin{displaymath}
  \Sigma=\text{diag}\left(\sigma_{1},\sigma_{2},\cdots,\sigma_{r}\right)\in\mathcal{R}^{m\times n},
\end{displaymath}
and $r=\min\left\{m,n\right\}$ is the rank of $\mathbf{A}$, with $\sigma_{1}\ge\sigma_{2}\ge\cdots\sigma_{r}\ge 0$. The diagonal entries of $\Sigma$ $\left(\text{\ie} \sigma_{i}, i=1,r\right)$ are called singular values of $\mathbf{A}$. If $\mathbf{A}$ is a square matrix then $\mathbf{U}$, $\Sigma$ and $\mathbf{V}$ are also square matrices. Also, as $\mathbf{U}$ and $\mathbf{V}$ are orthogonals and unitary, 
\begin{equation}
  \mathbf{U}^{\intercal}\mathbf{U} = \mathbf{V}^{\intercal}\mathbf{V} = \mathbf{I},
\end{equation}
where $\mathbf{I}$ is an identity matrix. However, if $\mathbf{A}$ is an $m\times n$ matrix, $\mathbf{U}$ and $\mathbf{V}$ still remain as square matrices, albeit with different matricial sizes. $\mathbf{U}$ is an $m\times m$ while $\mathbf{V}$ is an $n\times n$. Another relevant property of $\mathbf{U}$ and $\mathbf{V}$ since they are square (irrespective of whether $\mathbf{A}$ is square or rectangular) is that
\begin{equation}
  \mathbf{U}^{-1} = \mathbf{U}^{\intercal} \;\;\;\text{ and }\;\;\; \mathbf{V}^{-1} = \mathbf{V}^{\intercal},
\end{equation}
and if $\mathbf{A}$ is an $m \times n$ matrix thus $\Sigma$ is a diagonal $m \times n$ matrix.

\medskip
For a rectangular matrix, the singular values fill the first $r$ places on the main diagonal of $\Sigma$. The SVD benefits from the orthogonality of the columns in $\mathbf{U}$ and $\mathbf{V}$. The columns of $\mathbf{U}$ are known as the left singular vectors while the columns in $\mathbf{V}$ are known as the right singular vectors. A simple consideration of the benefit of the orthogonal factors, $\mathbf{U}$ and $\mathbf{V}$, is the ease with which they allow the inverse of the square matrix $\mathbf{A}$ to be calculated. This in turn makes it easy to solve the linear equations, $\mathbf{A}x = b$ with the SVD factorization,
\begin{equation}\label{e:SVD_factorization}
 \mathbf{U} \Sigma \mathbf{V}^{\intercal} x = b \;\;\Longrightarrow\;\; x = \mathbf{V} \Sigma^{-1} \mathbf{U}^{\intercal} b,
\end{equation} 
with benefit of the orthogonal property already discussed.


\section{\\A Brief introduction to HOSVD}\label{subsection:hosvd_brief}
Using the tensor notation from \citet{Kolda_2009}, given a tensor $\mathscr{X}\in\mathcal{R}^{N_{1}\times N_{2}\times \cdots \times N_{m}}$, the higher order singular value decomposition (HOSVD) is a method for factorizing $\mathscr{X}$, such that,
\begin{equation}
  \mathscr{X} = \mathscr{G} \times_{1} \mathbf{U}^{1}\times_{2} \mathbf{U}^{2}\times \cdots \times_{m} \mathbf{U}^{m}.
\end{equation}
where tensor $\mathscr{G}\in\mathcal{R}^{N_{1}\times N_{2}\times \cdots \times N_{m}}$ and is normally referred to the core tensor. Sub-tensors of $\mathscr{G}$ are orthogonal and ordered.\\
And matrices $\mathbf{U}^{m}\in\mathcal{R}^{N_{m}\times N_{m}}$ are orthogonal factor matrices which could be thought of as the principal components in each mode


\end{document}
%% End of tex file.



%%  LocalWords:  renormalization
